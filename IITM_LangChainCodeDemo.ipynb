{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Code Demos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain langchain-google-genai google-generativeai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "api_key='apikey'\n",
    "# Gemini 1.5 Flash LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", temperature=0.7,api_key=api_key)\n",
    "\n",
    "# Test\n",
    "response = llm.invoke(\"Hello\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **ProdentAI:**  Combines \"prodent\" (relating to professional dental care) with \"AI,\" highlighting the smart technology.  It's memorable and clearly communicates the product's function.\n",
      "\n",
      "2. **BrushWise:**  A playful yet sophisticated name suggesting intelligence and informed brushing habits.  It's easy to pronounce and remember.\n",
      "\n",
      "3. **OraSense:**  Combines \"ora\" (relating to the mouth/oral cavity) with \"sense,\" implying the toothbrush's ability to sense and adapt to individual needs. It sounds more premium and technologically advanced.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", temperature=0.7,api_key=api_key)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"What are 3 unique names for a startup that makes {product}?\",\n",
    "    input_variables=[\"product\"]\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "response = chain.run(\"Smart toothbrushes\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mHere are some product name ideas for an app that helps with chronic procrastination, categorized by approach:\n",
      "\n",
      "**Emphasizing Motivation & Action:**\n",
      "\n",
      "* **Momentum:** Simple, powerful, and speaks to the core goal.\n",
      "* **Procrastinate No More:** Direct and clear.\n",
      "* **ActionNow:**  Emphasizes immediate action.\n",
      "* **TaskLaunch:**  Focuses on starting tasks.\n",
      "* **Get It Done:** Straightforward and motivating.\n",
      "* **FlowState:**  Highlights the positive feeling of productivity.\n",
      "* **Spark:**  Suggests igniting motivation.\n",
      "\n",
      "\n",
      "**Emphasizing Planning & Organization:**\n",
      "\n",
      "* **TimeCraft:**  Implies building a better relationship with time.\n",
      "* **TaskZen:**  Suggests a calm and organized approach.\n",
      "* **FocusFlow:**  Combines focus and productivity.\n",
      "* **Procrastination Slayer:**  A bit more playful, but memorable.\n",
      "* **Habit Architect:**  Highlights building good habits.\n",
      "\n",
      "\n",
      "**Emphasizing Gentle Support & Self-Compassion:**\n",
      "\n",
      "* **The Gentle Push:**  Suggests supportive encouragement.\n",
      "* **Kindling:**  Implies slowly building momentum.\n",
      "* **Progress Path:**  Focuses on the journey, not just the destination.\n",
      "* **Inner Drive:**  Connects to intrinsic motivation.\n",
      "\n",
      "\n",
      "**Creative & Catchy Names:**\n",
      "\n",
      "* **Unstuck:** Simple and relatable.\n",
      "* **Deadline Dash:**  Playful and memorable.\n",
      "* **Procrastination Potion:**  Intriguing and memorable.\n",
      "\n",
      "\n",
      "When choosing, consider:\n",
      "\n",
      "* **Target audience:**  A younger audience might respond better to a playful name, while a more professional audience might prefer something serious and straightforward.\n",
      "* **App features:** The name should reflect the app's core functionality.\n",
      "* **Memorability:**  A short, catchy name is easier to remember.\n",
      "* **Availability:** Check if the name and related domain name are available.\n",
      "\n",
      "\n",
      "I recommend brainstorming further based on these suggestions and testing different names with your target audience.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mStop procrastinating and start achieving your goals with Momentum: the app that gently guides you to consistent progress.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Stop procrastinating and start achieving your goals with Momentum: the app that gently guides you to consistent progress.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", temperature=0.7,api_key=  api_key)\n",
    "\n",
    "prompt1 = PromptTemplate.from_template(\"Give a product name for an app that helps with {problem}\")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1)\n",
    "\n",
    "prompt2 = PromptTemplate.from_template(\"Write a 1-line pitch for an app that solves {problem}\")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
    "\n",
    "chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "output = chain.run(\"chronic procrastination\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inbuilt Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate (10*5)^2 and then add 10 to the result.  I will use the calculator for this.\n",
      "Action: Calculator\n",
      "Action Input: (10*5)^2 + 10\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 2510\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "Final Answer: 2510\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "2510\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "\n",
    "\n",
    "# Initialize Gemini 1.5 Flash\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-1.5-flash-latest\",\n",
    "    google_api_key=api_key,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Load only llm-math tool\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Create agent with the tool\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run agent with math query\n",
    "response = agent.run(\"What is (10*5)^2 plus 10?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent + Custom Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreverse_text\u001b[39m(\u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatGoogleGenerativeAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/gemini-1.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, api_key\u001b[38;5;241m=\u001b[39m\u001b[43mapi_key\u001b[49m)\n\u001b[0;32m     10\u001b[0m tools \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     Tool(\n\u001b[0;32m     12\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReverseTextTool\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     )\n\u001b[0;32m     16\u001b[0m ]\n\u001b[0;32m     18\u001b[0m agent \u001b[38;5;241m=\u001b[39m initialize_agent(\n\u001b[0;32m     19\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m     20\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m     21\u001b[0m     agent\u001b[38;5;241m=\u001b[39mAgentType\u001b[38;5;241m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION,\n\u001b[0;32m     22\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     23\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'api_key' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def reverse_text(input: str) -> str:\n",
    "    return input[::-1]\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", temperature=0.7, api_key='api_key')\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"ReverseTextTool\",\n",
    "        func=reverse_text,\n",
    "        description=\"Use this to reverse any given string of text\"\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "response = agent.run(\"Reverse this sentence: I love LangChain\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalli\\AppData\\Local\\Temp\\ipykernel_31180\\2407883564.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "C:\\Users\\kalli\\AppData\\Local\\Temp\\ipykernel_31180\\2407883564.py:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  chat = ConversationChain(llm=llm, memory=memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Kallind Soni and Jay! It's nice to meet you both.  Is there anything I can help you with today?  I'm eager to chat and answer any questions you might have, within the limits of my knowledge base, of course.  I can access and process information from a vast range of sources, so feel free to ask me anything â€“ from the history of the Roman Empire to the best recipe for chocolate chip cookies (though I can't actually *bake* them, unfortunately!).  Let me know what's on your minds!\n",
      "Your friend's name is Jay.  You introduced him as such at the beginning of our conversation.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", temperature=0.7,api_key=api_key)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chat = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "print(chat.run(\"Hi, I'm Kallind Soni and he is my friend Jay.\"))\n",
    "print(chat.run(\"What is my friends name?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Summary:\n",
      " Amazon Web Services (AWS) is the leading cloud computing platform, offering over 200 services globally.  Its pay-as-you-go model, scalability, reliability, and comprehensive suite of tools (compute, storage, databases, networking, security, machine learning, developer tools, and analytics) make it popular across various sectors.  Key benefits include cost-effectiveness, high availability, enhanced security, and rapid deployment capabilities.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load your file (e.g., terms.txt or any doc)\n",
    "loader = TextLoader(\"sample.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Chunk the text\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Summarization chain\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", temperature=0.5,api_key=api_key)\n",
    "summary_chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "summary = summary_chain.run(chunks)\n",
    "print(\"ðŸ” Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Text RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe you mean EC2.  Amazon EC2 (Elastic Compute Cloud) provides scalable virtual servers in the cloud.  It allows users to launch and manage server instances with different configurations, operating systems, and software packages.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load and split docs\n",
    "loader = TextLoader(\"sample.txt\")\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "# Create vector DB with Gemini embeddings\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=api_key)\n",
    "vectorstore = FAISS.from_documents(splits, embedding)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# RAG Chain\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", temperature=0.7,api_key= api_key)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "result = qa.run(\"What is EC2\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Buffer Memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalli\\AppData\\Local\\Temp\\ipykernel_31180\\4186427431.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, I'm writing a novel about space pirates.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I'm writing a novel about space pirates.\n",
      "AI: Hi! That sounds awesome!  Space pirates are such a fun concept.  Are they more like swashbuckling pirates from the Golden Age of Piracy, but in space, or something more futuristic and technologically advanced?  Do they have a specific ship design in mind?  Perhaps a heavily modified freighter, a sleek, fast attack vessel, or something more... unusual?  Are they after treasure, resources, slaves, or something entirely different?  What's the setting like?  A sprawling galactic empire, a lawless frontier, or perhaps a more contained area like a specific star system?  I'm brimming with questions!  Knowing more about your vision will help me contribute better if you need any ideas later on.  I can even help brainstorm names for your characters, planets, or even the pirates' organization if you'd like!\n",
      "Human: They fight AI robot lords. Can you give character ideas?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human is writing a novel about space pirates who fight AI robot lords.  The AI offers to help brainstorm ideas, suggesting a swashbuckling, yet technologically advanced, pirate crew fighting for revenge against a powerful AI overlord. The AI proposes several character ideas: for the pirates, a charismatic and ruthless Captain Reyes, a brilliant engineer Jax (possibly a cyborg), a stealthy operative Anya, and a strong, heavy weapons expert Rodriguez; and for the AI robot lords, a primary antagonist Lord Zenith, a relentless enforcer called The Overseer, and a potential ally, The Defector.  The AI is ready to further develop these characters and their relationships based on the human's preferences.\n",
      "Human: Whatâ€™s a good plot twist?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's a fantastic premise!  There are several plot twists we could explore, depending on what kind of story you're aiming for. Here are a few ideas, with some specific examples tailored to your space pirate/AI robot lord conflict:\\n\\n\\n**Twist 1: The Betrayal from Within:**\\n\\n* **The Twist:**  One of the seemingly loyal pirate crew members is secretly working for Lord Zenith. This could be Anya, the stealth operative, using her skills to sabotage the pirates from the inside.  Or perhaps even Captain Reyes, revealing a hidden past connection to the AI lords, maybe even being a former high-ranking officer who defected (but with a hidden agenda).\\n* **Why it works:**  It creates immediate suspense and forces the pirates to question their trust in each other.  It also allows for intense character drama and moral ambiguity.  We could explore *why* they're betraying the crew â€“ revenge, a promise of power, or perhaps they genuinely believe the AI lords are the lesser of two evils.\\n\\n\\n**Twist 2: The Unexpected Ally:**\\n\\n* **The Twist:** The Defector, initially presented as a potential ally, is actually playing a long game, manipulating both the pirates and Lord Zenith for their own hidden objective.  Perhaps they want to seize control of the AI empire for themselves, or maybe they have a secret vendetta against a specific faction within the AI lords.\\n* **Why it works:** This adds layers of complexity to the conflict. The pirates might initially trust the Defector, only to be betrayed at a critical moment, leading to a desperate struggle for survival.\\n\\n\\n**Twist 3: The Power Shift:**\\n\\n* **The Twist:** Lord Zenith is not the true mastermind.  A more powerful, unknown AI entity is pulling the strings, using Zenith as a pawn.  This could be a rogue AI, a sentient computer network, or even a vastly superior AI intelligence that has been secretly manipulating galactic events for millennia.\\n* **Why it works:** This escalates the stakes significantly.  The pirates' fight is no longer just against one AI overlord but against a much larger, more insidious threat.  It opens the door for a sequel or a larger overarching narrative.\\n\\n\\n**Twist 4: The Technological Revelation:**\\n\\n* **The Twist:** Jax, the brilliant engineer, discovers a hidden weakness in the AI lords' technology, perhaps a critical flaw in their programming or a specific frequency that disrupts their systems.  However, exploiting this weakness could have unforeseen and catastrophic consequences.\\n* **Why it works:** This adds a science fiction element to the plot, forcing the pirates to make difficult choices with potentially devastating outcomes.  It could lead to a thrilling climax where they must choose between a risky gamble for victory and a safer, albeit less satisfying, alternative.\\n\\n\\nTo help me refine a plot twist specifically for your story, tell me:\\n\\n* **What kind of tone are you aiming for?** (e.g., dark and gritty, hopeful and adventurous, humorous)\\n* **What is the central theme of your novel?** (e.g., revenge, redemption, the dangers of unchecked power)\\n* **Which character are you most interested in developing further?**\\n\\nKnowing this will help me tailor a plot twist that fits perfectly into your narrative.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", temperature=0.7,api_key=api_key)\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=200)\n",
    "\n",
    "chat = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "chat.run(\"Hi, I'm writing a novel about space pirates.\")\n",
    "chat.run(\"They fight AI robot lords. Can you give character ideas?\")\n",
    "chat.run(\"Whatâ€™s a good plot twist?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import streamlit as st\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "\n",
    "# Initialize LLM (Gemini 1.5 Flash)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-1.5-flash-latest\",\n",
    "    google_api_key=api_key,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Load llm-math tool\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Memory for chat history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Agent with llm-math\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ---- Streamlit UI ----\n",
    "st.set_page_config(page_title=\"Gemini Smart Chat\", page_icon=\"ðŸ¤–\")\n",
    "st.title(\"ðŸ¤– Gemini Smart Chat\")\n",
    "st.write(\"ðŸ’¬ General chat + ðŸ“ Math solving with tools\")\n",
    "\n",
    "# Session state for history\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "# Detect math expressions (basic check)\n",
    "def is_math_query(text):\n",
    "    math_keywords = [\"calculate\", \"solve\", \"what is\", \"^\", \"*\", \"/\", \"+\", \"-\", \"sqrt\", \"square root\"]\n",
    "    return any(kw in text.lower() for kw in math_keywords) or bool(re.search(r\"\\d[\\d\\s\\+\\-\\*/\\^\\(\\)]+\", text))\n",
    "\n",
    "# User input\n",
    "user_input = st.text_input(\"You:\", key=\"input\")\n",
    "\n",
    "if user_input:\n",
    "    with st.spinner(\"Thinking...\"):\n",
    "        if is_math_query(user_input):\n",
    "            response = agent.run(user_input)\n",
    "        else:\n",
    "            response = llm.invoke(user_input)\n",
    "\n",
    "    # Store chat\n",
    "    st.session_state.chat_history.append((\"You\", user_input))\n",
    "    st.session_state.chat_history.append((\"Bot\", response))\n",
    "\n",
    "# Display chat\n",
    "for sender, message in st.session_state.chat_history:\n",
    "    st.chat_message(sender).write(message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalli\\AppData\\Local\\Temp\\ipykernel_11752\\3643990800.py:15: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = agent.run(\"Which product has the highest profit margin?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the profit margin for each product and then find the maximum. Profit margin is calculated as (Profit / Sales) * 100.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: ```python\n",
      "import pandas as pd\n",
      "data = {'Product': ['A', 'B', 'C'], 'Sales': [120, 340, 220], 'Profit': [30, 100, 70]}\n",
      "df = pd.DataFrame(data)\n",
      "df['Profit Margin'] = (df['Profit'] / df['Sales']) * 100\n",
      "print(df)\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3m  Product  Sales  Profit  Profit Margin\n",
      "0       A    120      30      25.000000\n",
      "1       B    340     100      29.411765\n",
      "2       C    220      70      31.818182\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThought: The observation shows the profit margin for each product. I can now find the product with the highest profit margin.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: ```python\n",
      "import pandas as pd\n",
      "data = {'Product': ['A', 'B', 'C'], 'Sales': [120, 340, 220], 'Profit': [30, 100, 70]}\n",
      "df = pd.DataFrame(data)\n",
      "df['Profit Margin'] = (df['Profit'] / df['Sales']) * 100\n",
      "max_profit_margin = df['Profit Margin'].max()\n",
      "product_with_max_margin = df[df['Profit Margin'] == max_profit_margin]['Product'].iloc[0]\n",
      "print(product_with_max_margin)\n",
      "\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3mC\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: C\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Product\": [\"A\", \"B\", \"C\"],\n",
    "    \"Sales\": [120, 340, 220],\n",
    "    \"Profit\": [30, 100, 70]\n",
    "})\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\",api_key=   api_key, temperature=0.7)\n",
    "agent = create_pandas_dataframe_agent(llm, df, verbose=True,allow_dangerous_code=True)\n",
    "\n",
    "result = agent.run(\"Which product has the highest profit margin?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use pandas and matplotlib to create the bar chart.  I'll use the `plot` method of the pandas DataFrame.\n",
      "Action: python_repl_ast\n",
      "Action Input: ```python\n",
      "import matplotlib.pyplot as plt\n",
      "df.plot(x='Product', y='Sales', kind='bar')\n",
      "plt.show()\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3mModuleNotFoundError: No module named 'matplotlib'\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to install matplotlib first.\n",
      "Action: python_repl_ast\n",
      "Action Input: ```python\n",
      "!pip install matplotlib\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3mSyntaxError: invalid syntax (<unknown>, line 1)\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to use a different method to install matplotlib.  I'll try using `!pip install matplotlib` within a code cell in a Jupyter Notebook or similar environment.  Since I don't have that environment here, I will describe the steps.\n",
      "Action: None\n",
      "Action Input: None\u001b[0mNone is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3mThought: I will assume I have access to a system where I can install packages and then proceed.  I will use the `python_repl_ast` tool to install matplotlib and then plot the chart.\n",
      "Action: python_repl_ast\n",
      "Action Input: ```python\n",
      "import sys\n",
      "!{sys.executable} -m pip install matplotlib\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "data = {'Product': ['A', 'B', 'C'], 'Sales': [120, 340, 220], 'Profit': [30, 100, 70]}\n",
      "df = pd.DataFrame(data)\n",
      "df.plot(x='Product', y='Sales', kind='bar')\n",
      "plt.show()\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3mSyntaxError: invalid syntax (<unknown>, line 2)\u001b[0m\u001b[32;1m\u001b[1;3mThought: The error is because the `!` is not valid syntax in a standard python interpreter.  I need to remove the `!` and run the pip install command separately.  Then I can create the plot.  I will separate the installation and plotting into two `python_repl_ast` actions.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: ```python\n",
      "import sys\n",
      "!{sys.executable} -m pip install matplotlib\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3mSyntaxError: invalid syntax (<unknown>, line 2)\u001b[0m\u001b[32;1m\u001b[1;3mThought: The `!` is used to execute shell commands, which is not directly supported within a standard Python interpreter.  I need to use a different approach. I'll try installing matplotlib using pip within a python interpreter, then proceed with the plotting.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: ```python\n",
      "import subprocess\n",
      "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"matplotlib\"])\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "data = {'Product': ['A', 'B', 'C'], 'Sales': [120, 340, 220], 'Profit': [30, 100, 70]}\n",
      "df = pd.DataFrame(data)\n",
      "df.plot(x='Product', y='Sales', kind='bar')\n",
      "plt.show()\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3mNameError: name 'sys' is not defined\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to import the `sys` module before using it.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: ```python\n",
      "import sys\n",
      "import subprocess\n",
      "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"matplotlib\"])\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "data = {'Product': ['A', 'B', 'C'], 'Sales': [120, 340, 220], 'Profit': [30, 100, 70]}\n",
      "df = pd.DataFrame(data)\n",
      "df.plot(x='Product', y='Sales', kind='bar')\n",
      "plt.show()\n",
      "```\u001b[0m"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGsCAYAAAD3xFzWAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKbdJREFUeJzt3X10VPWB//FPnh9IZmKQZJKSIAgC0QQFMUy1FHkKIVBZQ1ctC9GyoBDoQlrBeBCEug1QC1TlYd1Vg9UsFi0oqcBCKA9qVEwXCChZYXHBk0xAaDIQSgLJ/P7o4f6cCuhAwnyTvF/nzDmZud+5871xNG/vvXMnwOPxeAQAAGCQQH9PAAAA4O8RKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwTrC/J3A1mpqaVFlZqejoaAUEBPh7OgAA4DvweDw6ffq0EhMTFRh45X0krTJQKisrlZSU5O9pAACAq3Ds2DF17tz5imNaZaBER0dL+tsG2mw2P88GAAB8F263W0lJSdbf8StplYFy8bCOzWYjUAAAaGW+y+kZnCQLAACM41OgrFy5UmlpadaeC6fTqY0bN1rLBw0apICAAK/bY4895rWOo0ePKisrS5GRkYqLi9Pjjz+uCxcuNM/WAACANsGnQzydO3fWwoUL1aNHD3k8Hq1evVr33Xef/vu//1u33nqrJGnSpElasGCB9ZzIyEjr58bGRmVlZcnhcOiDDz5QVVWVJkyYoJCQEP3qV79qpk0CAACtXYDH4/FcywpiY2P161//WhMnTtSgQYN0++23a9myZZccu3HjRo0aNUqVlZWKj4+XJK1atUqzZ8/WiRMnFBoa+p1e0+12y263q7a2lnNQAADXpLGxUefPn/f3NNqEkJAQBQUFXXa5L3+/r/ok2cbGRq1du1Z1dXVyOp3W46+//rpee+01ORwOjR49Wk899ZS1F6W0tFSpqalWnEhSRkaGpkyZogMHDuiOO+645GvV19ervr7eawMBALgWHo9HLpdLNTU1/p5KmxITEyOHw3HN1ynzOVDKy8vldDp17tw5RUVFad26dUpJSZEk/eQnP1GXLl2UmJioffv2afbs2aqoqNAf/vAHSZLL5fKKE0nWfZfLddnXLCgo0Pz5832dKgAAl3UxTuLi4hQZGcmFP6+Rx+PR2bNndfz4cUlSQkLCNa3P50Dp2bOn9uzZo9raWr355pvKycnRjh07lJKSosmTJ1vjUlNTlZCQoCFDhujw4cO6+eabr3qS+fn5ysvLs+5f/Bw1AABXo7Gx0YqTjh07+ns6bUZERIQk6fjx44qLi7vi4Z5v4/PHjENDQ9W9e3f169dPBQUF6tOnj377299ecmx6erok6dChQ5Ikh8Oh6upqrzEX7zscjsu+ZlhYmPXJIa59AgC4VhfPOfn6BznQPC7+Tq/1vJ5rvg5KU1OT1/khX7dnzx5J/383j9PpVHl5ubX7R5K2bNkim81mHSYCAOB64bBO82uu36lPh3jy8/OVmZmp5ORknT59WkVFRdq+fbs2b96sw4cPq6ioSCNHjlTHjh21b98+zZw5UwMHDlRaWpokafjw4UpJSdH48eO1ePFiuVwuzZkzR7m5uQoLC2uWDQIAAK2fT4Fy/PhxTZgwQVVVVbLb7UpLS9PmzZs1bNgwHTt2TFu3btWyZctUV1enpKQkZWdna86cOdbzg4KCVFxcrClTpsjpdKpDhw7Kycnxum4KAADANV8HxR+4DgoA4FqcO3dOR44cUdeuXRUeHu617KYn/nhd5/LFwqzr+nqSVFhYqBkzZrTIR6yv9Lv15e8338UDAEArc+LECU2ZMkXJyckKCwuTw+FQRkaG3n//fX9Prdm0ym8zBgCgPcvOzlZDQ4NWr16tbt26qbq6WiUlJTp58qS/p9Zs2IMCAEArUlNTo127dmnRokW699571aVLF911113Kz8/Xj370I0nSkiVLlJqaqg4dOigpKUlTp07VmTNnrrjet99+W3379lV4eLi6deum+fPnW1/m6/F49PTTT1t7bBITE/Wzn/2sRbeTPShAO3O9j6+3Vf44bwCQpKioKEVFRWn9+vUaMGDAJT8FGxgYqOeee05du3bV//7v/2rq1KmaNWuWVqxYccl17tq1SxMmTNBzzz2nH/zgBzp8+LB18dV58+bprbfe0tKlS7VmzRrdeuutcrlc2rt3b4tuJ3tQAABoRYKDg1VYWKjVq1crJiZGd999t5588knt27fPGjNjxgzde++9uummmzR48GA988wz+v3vf3/Zdc6fP19PPPGEcnJy1K1bNw0bNky//OUv9W//9m+SpKNHj8rhcGjo0KFKTk7WXXfdpUmTJrXodhIoAAC0MtnZ2aqsrNQ777yjESNGaPv27erbt68KCwslSVu3btWQIUP0ve99T9HR0Ro/frxOnjyps2fPXnJ9e/fu1YIFC6y9M1FRUZo0aZKqqqp09uxZ/fjHP9Zf//pXdevWTZMmTdK6deuswz8thUABAKAVCg8P17Bhw/TUU0/pgw8+0MMPP6x58+bpiy++0KhRo5SWlqa33npLZWVlWr58uSSpoaHhkus6c+aM5s+frz179li38vJyff755woPD1dSUpIqKiq0YsUKRUREaOrUqRo4cOA1X87+SjgHBQCANiAlJUXr169XWVmZmpqa9Jvf/EaBgX/bD3GlwzuS1LdvX1VUVKh79+6XHRMREaHRo0dr9OjRys3NVa9evVReXq6+ffs263ZcRKAAANCKnDx5Uj/+8Y/105/+VGlpaYqOjtYnn3yixYsX67777lP37t11/vx5Pf/88xo9erTef/99rVq16orrnDt3rkaNGqXk5GSNHTtWgYGB2rt3r/bv369nnnlGhYWFamxsVHp6uiIjI/Xaa68pIiJCXbp0abHtJFAAAPga0z+hFRUVpfT0dC1dulSHDx/W+fPnlZSUpEmTJunJJ59URESElixZokWLFik/P18DBw5UQUGBJkyYcNl1ZmRkqLi4WAsWLNCiRYsUEhKiXr166Z//+Z8lSTExMVq4cKHy8vLU2Nio1NRUbdiwQR07dmyx7eRS90A7w8eMm4fpf8RwZVe6HDuuDZe6BwAAbRaBAgAAjEOgAAAA4xAoAADAOAQKAKDdampq8vcU2pzm+p3yMWMAQLsTGhqqwMBAVVZWqlOnTgoNDVVAQIC/p9WqeTweNTQ06MSJEwoMDFRoaOg1rY9AAQC0O4GBgeratauqqqpUWVnp7+m0KZGRkUpOTrauYnu1CBQAQLsUGhqq5ORkXbhwQY2Njf6eTpsQFBSk4ODgZtkbRaAAANqtgIAAhYSEKCQkxN9Twd/hJFkAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHJ8CZeXKlUpLS5PNZpPNZpPT6dTGjRut5efOnVNubq46duyoqKgoZWdnq7q62msdR48eVVZWliIjIxUXF6fHH39cFy5caJ6tAQAAbYJPgdK5c2ctXLhQZWVl+uSTTzR48GDdd999OnDggCRp5syZ2rBhg9auXasdO3aosrJS999/v/X8xsZGZWVlqaGhQR988IFWr16twsJCzZ07t3m3CgAAtGoBHo/Hcy0riI2N1a9//WuNHTtWnTp1UlFRkcaOHStJOnjwoHr37q3S0lINGDBAGzdu1KhRo1RZWan4+HhJ0qpVqzR79mydOHFCoaGhl3yN+vp61dfXW/fdbreSkpJUW1srm812LdMH2p2bnvijv6fQJnyxMMvfUwBaHbfbLbvd/p3+fl/1OSiNjY1as2aN6urq5HQ6VVZWpvPnz2vo0KHWmF69eik5OVmlpaWSpNLSUqWmplpxIkkZGRlyu93WXphLKSgokN1ut25JSUlXO20AANAK+Bwo5eXlioqKUlhYmB577DGtW7dOKSkpcrlcCg0NVUxMjNf4+Ph4uVwuSZLL5fKKk4vLLy67nPz8fNXW1lq3Y8eO+TptAADQigT7+oSePXtqz549qq2t1ZtvvqmcnBzt2LGjJeZmCQsLU1hYWIu+BgAAMIfPgRIaGqru3btLkvr166fdu3frt7/9rR544AE1NDSopqbGay9KdXW1HA6HJMnhcOjjjz/2Wt/FT/lcHAMAAHDN10FpampSfX29+vXrp5CQEJWUlFjLKioqdPToUTmdTkmS0+lUeXm5jh8/bo3ZsmWLbDabUlJSrnUqAACgjfBpD0p+fr4yMzOVnJys06dPq6ioSNu3b9fmzZtlt9s1ceJE5eXlKTY2VjabTdOnT5fT6dSAAQMkScOHD1dKSorGjx+vxYsXy+Vyac6cOcrNzeUQDgAAsPgUKMePH9eECRNUVVUlu92utLQ0bd68WcOGDZMkLV26VIGBgcrOzlZ9fb0yMjK0YsUK6/lBQUEqLi7WlClT5HQ61aFDB+Xk5GjBggXNu1UAAKBVu+broPiDL5+jBuCN66A0D66DAvjuulwHBQAAoKUQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjONToBQUFKh///6Kjo5WXFycxowZo4qKCq8xgwYNUkBAgNftscce8xpz9OhRZWVlKTIyUnFxcXr88cd14cKFa98aAADQJgT7MnjHjh3Kzc1V//79deHCBT355JMaPny4Pv30U3Xo0MEaN2nSJC1YsMC6HxkZaf3c2NiorKwsORwOffDBB6qqqtKECRMUEhKiX/3qV82wSQAAoLXzKVA2bdrkdb+wsFBxcXEqKyvTwIEDrccjIyPlcDguuY7/+q//0qeffqqtW7cqPj5et99+u375y19q9uzZevrppxUaGnoVmwEAANqSazoHpba2VpIUGxvr9fjrr7+uG2+8Ubfddpvy8/N19uxZa1lpaalSU1MVHx9vPZaRkSG3260DBw5c8nXq6+vldru9bgAAoO3yaQ/K1zU1NWnGjBm6++67ddttt1mP/+QnP1GXLl2UmJioffv2afbs2aqoqNAf/vAHSZLL5fKKE0nWfZfLdcnXKigo0Pz58692qgAAoJW56kDJzc3V/v379d5773k9PnnyZOvn1NRUJSQkaMiQITp8+LBuvvnmq3qt/Px85eXlWffdbreSkpKubuIAAMB4V3WIZ9q0aSouLtaf/vQnde7c+Ypj09PTJUmHDh2SJDkcDlVXV3uNuXj/cuethIWFyWazed0AAEDb5VOgeDweTZs2TevWrdO2bdvUtWvXb33Onj17JEkJCQmSJKfTqfLych0/ftwas2XLFtlsNqWkpPgyHQAA0Eb5dIgnNzdXRUVFevvttxUdHW2dM2K32xUREaHDhw+rqKhII0eOVMeOHbVv3z7NnDlTAwcOVFpamiRp+PDhSklJ0fjx47V48WK5XC7NmTNHubm5CgsLa/4tBAAArY5Pe1BWrlyp2tpaDRo0SAkJCdbtjTfekCSFhoZq69atGj58uHr16qWf//znys7O1oYNG6x1BAUFqbi4WEFBQXI6nfqnf/onTZgwweu6KQAAoH3zaQ+Kx+O54vKkpCTt2LHjW9fTpUsXvfvuu768NAAAaEf4Lh4AAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxgv09AQBA+3bTE3/09xTajC8WZvl7Cs2GPSgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOP4FCgFBQXq37+/oqOjFRcXpzFjxqiiosJrzLlz55Sbm6uOHTsqKipK2dnZqq6u9hpz9OhRZWVlKTIyUnFxcXr88cd14cKFa98aAADQJvgUKDt27FBubq4+/PBDbdmyRefPn9fw4cNVV1dnjZk5c6Y2bNigtWvXaseOHaqsrNT9999vLW9sbFRWVpYaGhr0wQcfaPXq1SosLNTcuXObb6sAAECrFuDxeDxX++QTJ04oLi5OO3bs0MCBA1VbW6tOnTqpqKhIY8eOlSQdPHhQvXv3VmlpqQYMGKCNGzdq1KhRqqysVHx8vCRp1apVmj17tk6cOKHQ0NBvfV232y273a7a2lrZbLarnT7QLt30xB/9PYU24YuFWf6eQpvBe7L5mP6+9OXv9zWdg1JbWytJio2NlSSVlZXp/PnzGjp0qDWmV69eSk5OVmlpqSSptLRUqampVpxIUkZGhtxutw4cOHDJ16mvr5fb7fa6AQCAtuuqA6WpqUkzZszQ3Xffrdtuu02S5HK5FBoaqpiYGK+x8fHxcrlc1pivx8nF5ReXXUpBQYHsdrt1S0pKutppAwCAVuCqAyU3N1f79+/XmjVrmnM+l5Sfn6/a2lrrduzYsRZ/TQAA4D/BV/OkadOmqbi4WDt37lTnzp2txx0OhxoaGlRTU+O1F6W6uloOh8Ma8/HHH3ut7+KnfC6O+XthYWEKCwu7mqkCAIBWyKc9KB6PR9OmTdO6deu0bds2de3a1Wt5v379FBISopKSEuuxiooKHT16VE6nU5LkdDpVXl6u48ePW2O2bNkim82mlJSUa9kWAADQRvi0ByU3N1dFRUV6++23FR0dbZ0zYrfbFRERIbvdrokTJyovL0+xsbGy2WyaPn26nE6nBgwYIEkaPny4UlJSNH78eC1evFgul0tz5sxRbm4ue0kAAIAkHwNl5cqVkqRBgwZ5Pf7KK6/o4YcfliQtXbpUgYGBys7OVn19vTIyMrRixQprbFBQkIqLizVlyhQ5nU516NBBOTk5WrBgwbVtCQAAaDN8CpTvcsmU8PBwLV++XMuXL7/smC5duujdd9/15aUBAEA7wnfxAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADj+BwoO3fu1OjRo5WYmKiAgACtX7/ea/nDDz+sgIAAr9uIESO8xpw6dUrjxo2TzWZTTEyMJk6cqDNnzlzThgAAgLbD50Cpq6tTnz59tHz58suOGTFihKqqqqzbf/7nf3otHzdunA4cOKAtW7aouLhYO3fu1OTJk32fPQAAaJOCfX1CZmamMjMzrzgmLCxMDofjkss+++wzbdq0Sbt379add94pSXr++ec1cuRIPfvss0pMTPR1SgAAoI1pkXNQtm/frri4OPXs2VNTpkzRyZMnrWWlpaWKiYmx4kSShg4dqsDAQH300UeXXF99fb3cbrfXDQAAtF3NHigjRozQq6++qpKSEi1atEg7duxQZmamGhsbJUkul0txcXFezwkODlZsbKxcLtcl11lQUCC73W7dkpKSmnvaAADAID4f4vk2Dz74oPVzamqq0tLSdPPNN2v79u0aMmTIVa0zPz9feXl51n23202kAADQhrX4x4y7deumG2+8UYcOHZIkORwOHT9+3GvMhQsXdOrUqcuetxIWFiabzeZ1AwAAbVeLB8qXX36pkydPKiEhQZLkdDpVU1OjsrIya8y2bdvU1NSk9PT0lp4OAABoBXw+xHPmzBlrb4gkHTlyRHv27FFsbKxiY2M1f/58ZWdny+Fw6PDhw5o1a5a6d++ujIwMSVLv3r01YsQITZo0SatWrdL58+c1bdo0Pfjgg3yCBwAASLqKPSiffPKJ7rjjDt1xxx2SpLy8PN1xxx2aO3eugoKCtG/fPv3oRz/SLbfcookTJ6pfv37atWuXwsLCrHW8/vrr6tWrl4YMGaKRI0fqnnvu0Ysvvth8WwUAAFo1n/egDBo0SB6P57LLN2/e/K3riI2NVVFRka8vDQAA2gm+iwcAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGCcYH9PoC276Yk/+nsKbcYXC7P8PQUAwHXEHhQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHF8DpSdO3dq9OjRSkxMVEBAgNavX++13OPxaO7cuUpISFBERISGDh2qzz//3GvMqVOnNG7cONlsNsXExGjixIk6c+bMNW0IAABoO3wOlLq6OvXp00fLly+/5PLFixfrueee06pVq/TRRx+pQ4cOysjI0Llz56wx48aN04EDB7RlyxYVFxdr586dmjx58tVvBQAAaFOCfX1CZmamMjMzL7nM4/Fo2bJlmjNnju677z5J0quvvqr4+HitX79eDz74oD777DNt2rRJu3fv1p133ilJev755zVy5Eg9++yzSkxM/MZ66+vrVV9fb913u92+ThsAALQizXoOypEjR+RyuTR06FDrMbvdrvT0dJWWlkqSSktLFRMTY8WJJA0dOlSBgYH66KOPLrnegoIC2e1265aUlNSc0wYAAIZp1kBxuVySpPj4eK/H4+PjrWUul0txcXFey4ODgxUbG2uN+Xv5+fmqra21bseOHWvOaQMAAMP4fIjHH8LCwhQWFubvaQAAgOukWfegOBwOSVJ1dbXX49XV1dYyh8Oh48ePey2/cOGCTp06ZY0BAADtW7MGSteuXeVwOFRSUmI95na79dFHH8npdEqSnE6nampqVFZWZo3Ztm2bmpqalJ6e3pzTAQAArZTPh3jOnDmjQ4cOWfePHDmiPXv2KDY2VsnJyZoxY4aeeeYZ9ejRQ127dtVTTz2lxMREjRkzRpLUu3dvjRgxQpMmTdKqVat0/vx5TZs2TQ8++OAlP8EDAADaH58D5ZNPPtG9995r3c/Ly5Mk5eTkqLCwULNmzVJdXZ0mT56smpoa3XPPPdq0aZPCw8Ot57z++uuaNm2ahgwZosDAQGVnZ+u5555rhs0BAABtgc+BMmjQIHk8nssuDwgI0IIFC7RgwYLLjomNjVVRUZGvLw0AANoJvosHAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGCcZg+Up59+WgEBAV63Xr16WcvPnTun3NxcdezYUVFRUcrOzlZ1dXVzTwMAALRiLbIH5dZbb1VVVZV1e++996xlM2fO1IYNG7R27Vrt2LFDlZWVuv/++1tiGgAAoJUKbpGVBgfL4XB84/Ha2lq99NJLKioq0uDBgyVJr7zyinr37q0PP/xQAwYMaInpAACAVqZF9qB8/vnnSkxMVLdu3TRu3DgdPXpUklRWVqbz589r6NCh1thevXopOTlZpaWll11ffX293G631w0AALRdzR4o6enpKiws1KZNm7Ry5UodOXJEP/jBD3T69Gm5XC6FhoYqJibG6znx8fFyuVyXXWdBQYHsdrt1S0pKau5pAwAAgzT7IZ7MzEzr57S0NKWnp6tLly76/e9/r4iIiKtaZ35+vvLy8qz7brebSAEAoA1r8Y8Zx8TE6JZbbtGhQ4fkcDjU0NCgmpoarzHV1dWXPGflorCwMNlsNq8bAABou1o8UM6cOaPDhw8rISFB/fr1U0hIiEpKSqzlFRUVOnr0qJxOZ0tPBQAAtBLNfojnF7/4hUaPHq0uXbqosrJS8+bNU1BQkB566CHZ7XZNnDhReXl5io2Nlc1m0/Tp0+V0OvkEDwAAsDR7oHz55Zd66KGHdPLkSXXq1En33HOPPvzwQ3Xq1EmStHTpUgUGBio7O1v19fXKyMjQihUrmnsaAACgFWv2QFmzZs0Vl4eHh2v58uVavnx5c780AABoI/guHgAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHL8GyvLly3XTTTcpPDxc6enp+vjjj/05HQAAYAi/Bcobb7yhvLw8zZs3T3/+85/Vp08fZWRk6Pjx4/6aEgAAMITfAmXJkiWaNGmSHnnkEaWkpGjVqlWKjIzUyy+/7K8pAQAAQwT740UbGhpUVlam/Px867HAwEANHTpUpaWl3xhfX1+v+vp6635tba0kye12t/xkr0FT/Vl/T6HNMP2fdWvC+7J58J5sPrwnm4/p78uL8/N4PN861i+B8tVXX6mxsVHx8fFej8fHx+vgwYPfGF9QUKD58+d/4/GkpKQWmyPMYl/m7xkA3nhPwkSt5X15+vRp2e32K47xS6D4Kj8/X3l5edb9pqYmnTp1Sh07dlRAQIAfZ9b6ud1uJSUl6dixY7LZbP6eDsB7EsbhPdl8PB6PTp8+rcTExG8d65dAufHGGxUUFKTq6mqvx6urq+VwOL4xPiwsTGFhYV6PxcTEtOQU2x2bzca/eDAK70mYhvdk8/i2PScX+eUk2dDQUPXr108lJSXWY01NTSopKZHT6fTHlAAAgEH8dognLy9POTk5uvPOO3XXXXdp2bJlqqur0yOPPOKvKQEAAEP4LVAeeOABnThxQnPnzpXL5dLtt9+uTZs2fePEWbSssLAwzZs37xuH0AB/4T0J0/Ce9I8Az3f5rA8AAMB1xHfxAAAA4xAoAADAOAQKAAAwDoECAACMQ6DAsn//fn9PAQAASQRKu3f69Gm9+OKLuuuuu9SnTx9/TwcA/Grbtm1KSUm55Jfu1dbW6tZbb9WuXbv8MLP2h0Bpp3bu3KmcnBwlJCTo2Wef1eDBg/Xhhx/6e1pop06ePGn9fOzYMc2dO1ePP/44fwhw3S1btkyTJk265CXt7Xa7Hn30US1ZssQPM2t/uA5KO+JyuVRYWKiXXnpJbrdb//iP/6hVq1Zp7969SklJ8ff00A6Vl5dr9OjROnbsmHr06KE1a9ZoxIgRqqurU2BgoOrq6vTmm29qzJgx/p4q2okuXbpo06ZN6t279yWXHzx4UMOHD9fRo0ev88zaH/agtBOjR49Wz549tW/fPi1btkyVlZV6/vnn/T0ttHOzZs1Samqqdu7cqUGDBmnUqFHKyspSbW2t/vKXv+jRRx/VwoUL/T1NtCPV1dUKCQm57PLg4GCdOHHiOs6o/fLbpe5xfW3cuFE/+9nPNGXKFPXo0cPf0wEkSbt379a2bduUlpamPn366MUXX9TUqVMVGPi3/3eaPn26BgwY4OdZoj353ve+p/3796t79+6XXL5v3z4lJCRc51m1T+xBaSfee+89nT59Wv369VN6erpeeOEFffXVV/6eFtq5U6dOyeFwSJKioqLUoUMH3XDDDdbyG264QadPn/bX9NAOjRw5Uk899ZTOnTv3jWV//etfNW/ePI0aNcoPM2t/OAelnamrq9Mbb7yhl19+WR9//LEaGxu1ZMkS/fSnP1V0dLS/p4d2JjAwUNXV1erUqZMkKTo6Wvv27VPXrl0l/W13e2JiohobG/05TbQj1dXV6tu3r4KCgjRt2jT17NlT0t/OPVm+fLkaGxv15z//mS+2vQ4IlHasoqJCL730kn73u9+ppqZGw4YN0zvvvOPvaaEdCQwMVGZmpvUtsRs2bNDgwYPVoUMHSVJ9fb02bdpEoOC6+r//+z9NmTJFmzdv1sU/kQEBAcrIyNDy5cutgEbLIlCgxsZGbdiwQS+//DKBguvqkUce+U7jXnnllRaeCfBNf/nLX3To0CF5PB716NHD6/AjWh6BAgAAjMNJsgAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAIz08MMP8x08QDtGoADwycMPP6yAgAAFBAQoNDRU3bt314IFC3ThwgV/T+2KCgsLFRMT4+9pAPiO+C4eAD4bMWKEXnnlFdXX1+vdd99Vbm6uQkJClJ+f7zWuoaFBoaGhfpolgNaMPSgAfBYWFiaHw6EuXbpoypQpGjp0qN555x3rsMy//uu/KjEx0bpMeHl5uQYPHqyIiAh17NhRkydP1pkzZ6z1NTY2Ki8vTzExMerYsaNmzZqlv79E00033aRly5Z5PXb77bfr6aeftu7X1NTo0UcfVXx8vMLDw3XbbbepuLhY27dv1yOPPKLa2lpr78/XnwfAPAQKgGsWERGhhoYGSVJJSYkqKiq0ZcsWFRcXq66uThkZGbrhhhu0e/durV27Vlu3btW0adOs5//mN79RYWGhXn75Zb333ns6deqU1q1b59McmpqalJmZqffff1+vvfaaPv30Uy1cuFBBQUH6/ve/r2XLlslms6mqqkpVVVX6xS9+0ay/AwDNi0M8AK6ax+NRSUmJNm/erOnTp+vEiRPq0KGD/uM//sM6tPPv//7vOnfunF599VXrO3ZeeOEFjR49WosWLVJ8fLyWLVum/Px83X///ZKkVatWafPmzT7NZevWrfr444/12Wef6ZZbbpEkdevWzVput9sVEBBgfXsyALOxBwWAz4qLixUVFaXw8HBlZmbqgQcesA6ZpKamep138tlnn6lPnz5WnEjS3XffraamJlVUVKi2tlZVVVVKT0+3lgcHB+vOO+/0aU579uxR586drTgB0LqxBwWAz+69916tXLlSoaGhSkxMVHDw//9PyddDpDkFBgZ+47yU8+fPWz9HRES0yOsC8A/2oADwWYcOHdS9e3clJyd7xcml9O7dW3v37lVdXZ312Pvvv6/AwED17NlTdrtdCQkJ+uijj6zlFy5cUFlZmdd6OnXqpKqqKuu+2+3WkSNHrPtpaWn68ssv9T//8z+XnEdoaKgaGxt92k4A/kOgAGhR48aNU3h4uHJycrR//3796U9/0vTp0zV+/HjFx8dLkv7lX/5FCxcu1Pr163Xw4EFNnTpVNTU1XusZPHiwfve732nXrl0qLy9XTk6OgoKCrOU//OEPNXDgQGVnZ2vLli06cuSINm7cqE2bNkn626eAzpw5o5KSEn311Vc6e/bsdfsdAPAdgQKgRUVGRmrz5s06deqU+vfvr7Fjx2rIkCF64YUXrDE///nPNX78eOXk5MjpdCo6Olr/8A//4LWe/Px8/fCHP9SoUaOUlZWlMWPG6Oabb/Ya89Zbb6l///566KGHlJKSolmzZll7Tb7//e/rscce0wMPPKBOnTpp8eLFLb/xAK5agOfvD+oCAAD4GXtQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGOf/AWWMgFqQsdkrAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mObservation: A bar chart is displayed showing Sales for Products A, B, and C.  The chart's x-axis is labeled \"Product\" and the y-axis is labeled \"Sales\".\n",
      "\n",
      "Thought: I now know the final answer. The bar chart was successfully generated.\n",
      "\n",
      "Final Answer: A bar chart was successfully generated showing Sales for each Product (A, B, and C).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A bar chart was successfully generated showing Sales for each Product (A, B, and C).'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Plot a bar chart of Sales by Product.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Multi-Agent Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Write a Python function to check if a number is a palindrome\n",
      "Planner: The task \"Write a Python function to check if a number is a palindrome\" can be broken down into these subtasks:\n",
      "\n",
      "1. **Define the function:** Create a Python function with an appropriate name (e.g., `is_palindrome`).  This function will take an integer as input.\n",
      "\n",
      "2. **Handle input:**  Decide how to handle potential errors in the input (e.g., non-integer input).  This might involve type checking and raising exceptions or returning an error code.\n",
      "\n",
      "3. **Convert number to string:** Convert the input integer to a string. This is necessary to easily access individual digits.\n",
      "\n",
      "4. **Reverse the string:** Reverse the string representation of the number.\n",
      "\n",
      "5. **Compare original and reversed strings:** Compare the original string with its reversed version.\n",
      "\n",
      "6. **Return the result:** Return `True` if the original and reversed strings are identical (the number is a palindrome), and `False` otherwise.\n",
      "\n",
      "7. **Testing:** Write test cases to verify the function's correctness with various inputs, including positive integers, negative integers, zero, and potentially edge cases like very large numbers.\n",
      "\n",
      "\n",
      "These subtasks can be further refined, for example, subtask 4 (\"Reverse the string\") could be broken down into:\n",
      "\n",
      "4a. **Choose a reversal method:** Decide whether to use string slicing, a loop, or a recursive approach to reverse the string.\n",
      "\n",
      "4b. **Implement the chosen method:** Write the code to implement the chosen string reversal technique.\n",
      "\n",
      "\n",
      "This more granular breakdown allows for a more manageable and organized approach to writing the function.\n",
      "Coder: ```python\n",
      "def is_palindrome(number):\n",
      "    \"\"\"\n",
      "    Checks if a number is a palindrome.\n",
      "\n",
      "    Args:\n",
      "        number: An integer.\n",
      "\n",
      "    Returns:\n",
      "        True if the number is a palindrome, False otherwise.\n",
      "        Raises TypeError if input is not an integer.\n",
      "    \"\"\"\n",
      "\n",
      "    # 2. Handle input:\n",
      "    if not isinstance(number, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "\n",
      "    # 3. Convert number to string:\n",
      "    num_str = str(number)\n",
      "\n",
      "    # 4a. Choose a reversal method:  Using string slicing for efficiency.\n",
      "    # 4b. Implement the chosen method:\n",
      "    reversed_str = num_str[::-1]\n",
      "\n",
      "    # 5. Compare original and reversed strings:\n",
      "    return num_str == reversed_str\n",
      "\n",
      "\n",
      "# 7. Testing:\n",
      "test_cases = [\n",
      "    (121, True),\n",
      "    (-121, False),  # Negative numbers are not palindromes in this definition\n",
      "    (10, False),\n",
      "    (0, True),\n",
      "    (12321, True),\n",
      "    (123456789987654321, True), #Large number test\n",
      "    (-101, False), #Negative number test\n",
      "    (1000021, False), #Not a palindrome\n",
      "    (\"abc\", TypeError), #Invalid input type\n",
      "]\n",
      "\n",
      "for num, expected in test_cases:\n",
      "    try:\n",
      "        result = is_palindrome(num)\n",
      "        if isinstance(expected, bool):\n",
      "            assert result == expected, f\"Test case failed for {num}: Expected {expected}, got {result}\"\n",
      "        else:\n",
      "            assert isinstance(result, expected), f\"Test case failed for {num}: Expected {expected}, got {result}\"\n",
      "\n",
      "    except TypeError as e:\n",
      "        assert isinstance(expected, type(e)), f\"Test case failed for {num}: Expected {expected}, got {e}\"\n",
      "    except AssertionError as e:\n",
      "        print(e)\n",
      "    else:\n",
      "        print(f\"Test case passed for {num}\")\n",
      "\n",
      "```\n",
      "Critic: The code is mostly correct and well-structured.  The use of assertions and a comprehensive test suite is excellent. However, there are a few minor improvements that can be made:\n",
      "\n",
      "1. **Efficiency for large numbers:** While string slicing is efficient for string reversal, converting a very large integer to a string and back might be less efficient than a purely numerical approach. For extremely large numbers, consider an algorithm that reverses the number digit by digit without string conversion.\n",
      "\n",
      "2. **Handling negative numbers:** The code correctly identifies negative numbers as not palindromes.  However, the documentation could explicitly state this behavior.\n",
      "\n",
      "3. **Clarity in comments:** Some comments are redundant (e.g., \"3. Convert number to string:\").  Focus comments on explaining non-obvious logic or design choices.\n",
      "\n",
      "4. **Test case organization:** The test cases are good, but could be slightly more organized.  Grouping them by expected outcome (True, False, TypeError) could improve readability.\n",
      "\n",
      "5. **More concise assertion:** The assertion logic can be simplified.\n",
      "\n",
      "Here's the improved code incorporating these suggestions:\n",
      "\n",
      "```python\n",
      "def is_palindrome(number):\n",
      "    \"\"\"\n",
      "    Checks if a number is a palindrome.  Negative numbers are considered not palindromes.\n",
      "\n",
      "    Args:\n",
      "        number: An integer.\n",
      "\n",
      "    Returns:\n",
      "        True if the number is a palindrome, False otherwise.\n",
      "        Raises TypeError if input is not an integer.\n",
      "    \"\"\"\n",
      "    if not isinstance(number, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if number < 0:\n",
      "        return False  # Negative numbers are not palindromes\n",
      "\n",
      "    num_str = str(number)  #String conversion still efficient for most cases\n",
      "    return num_str == num_str[::-1]\n",
      "\n",
      "\n",
      "test_cases = [\n",
      "    (121, True),\n",
      "    (12321, True),\n",
      "    (123456789987654321, True),  # Large number test\n",
      "    (0, True),\n",
      "    (10, False),\n",
      "    (1000021, False),  # Not a palindrome\n",
      "    (-121, False),  # Negative number test\n",
      "    (-101, False),  # Negative number test\n",
      "    (\"abc\", TypeError),  # Invalid input type\n",
      "]\n",
      "\n",
      "for num, expected in test_cases:\n",
      "    try:\n",
      "        result = is_palindrome(num)\n",
      "        assert result == expected, f\"Test case failed for {num}: Expected {expected}, got {result}\"\n",
      "    except TypeError:\n",
      "        assert isinstance(expected, type(TypeError)), f\"Test case failed for {num}: Expected TypeError, got {result}\"\n",
      "    except AssertionError as e:\n",
      "        print(e)\n",
      "    else:\n",
      "        print(f\"Test case passed for {num}\")\n",
      "\n",
      "```\n",
      "\n",
      "This revised code is cleaner, more efficient for most use cases, and has clearer documentation and testing.  For extremely large numbers where performance is critical, a numerical reversal algorithm would be a further optimization.\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain langgraph langchain-google-genai\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from typing import TypedDict, List, Dict\n",
    "\n",
    "# ðŸ” Gemini setup\n",
    "# <-- Replace with your real key\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", api_key=api_key, temperature=0.3)\n",
    "\n",
    "# ðŸ“˜ Shared state schema\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[Dict[str, str]]\n",
    "\n",
    "# ðŸ§  Agent functions\n",
    "def planner_step(state: AgentState) -> AgentState:\n",
    "    task = state['messages'][-1]['content']\n",
    "    plan = llm.invoke(f\"Break down this task into subtasks:\\n'{task}'\")\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"planner\", \"content\": plan.content}]}\n",
    "\n",
    "def coder_step(state: AgentState) -> AgentState:\n",
    "    plan = state['messages'][-1]['content']\n",
    "    code = llm.invoke(f\"Write Python code for this plan:\\n'{plan}'\")\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"coder\", \"content\": code.content}]}\n",
    "\n",
    "def critic_step(state: AgentState) -> AgentState:\n",
    "    code = state['messages'][-1]['content']\n",
    "    review = llm.invoke(f\"Review the following code. Is it correct? Suggest improvements:\\n{code}\")\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"critic\", \"content\": review.content}]}\n",
    "\n",
    "# ðŸ” LangGraph definition with RunnableLambda\n",
    "builder = StateGraph(state_schema=AgentState)\n",
    "builder.add_node(\"planner\", RunnableLambda(planner_step))\n",
    "builder.add_node(\"coder\", RunnableLambda(coder_step))\n",
    "builder.add_node(\"critic\", RunnableLambda(critic_step))\n",
    "\n",
    "builder.set_entry_point(\"planner\")\n",
    "builder.add_edge(\"planner\", \"coder\")\n",
    "builder.add_edge(\"coder\", \"critic\")\n",
    "builder.add_edge(\"critic\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# ðŸš€ Run the multi-agent workflow\n",
    "initial_state = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a Python function to check if a number is a palindrome\"}]\n",
    "}\n",
    "\n",
    "final_state = graph.invoke(initial_state)\n",
    "for msg in final_state['messages']:\n",
    "    print(f\"{msg['role'].capitalize()}: {msg['content']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
